# app_openalex_streamlit_relaxed.py
import os
import re
from datetime import datetime
from urllib.parse import urlencode

import requests
import pandas as pd
import matplotlib.pyplot as plt
import streamlit as st
from jinja2 import Template

# ================== UI Config ==================
st.set_page_config(page_title="Auto Paper (OpenAlex + Gemini)", layout="wide")
st.title("üß™ Auto Paper: OpenAlex ‚Üí (t√πy ch·ªçn) Gemini vi·∫øt b√†i")

with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh t√¨m ki·∫øm (OpenAlex)")
    topic = st.text_input("Ch·ªß ƒë·ªÅ", "tƒÉng tr∆∞·ªüng xanh v√† chuy·ªÉn d·ªãch nƒÉng l∆∞·ª£ng")
    year_range = st.text_input("Kho·∫£ng nƒÉm (YYYY-YYYY)", "2000-2025")
    per_page = st.number_input("S·ªë m·ª•c m·ªói trang", 10, 200, 100)
    max_pages = st.number_input("S·ªë trang t·ªëi ƒëa", 1, 20, 8)
    max_sources = st.number_input("Gi·ªõi h·∫°n ngu·ªìn ƒë·∫ßu ra", 10, 500, 100)
    verify_doi = st.checkbox("X√°c th·ª±c DOI (HEAD t·ªõi doi.org, c√≥ th·ªÉ ch·∫≠m)", False)
    loosen_types = st.checkbox("N·ªõi l·ªèng lo·∫°i t√†i li·ªáu (journal|proceedings|report|book-chapter)", True)
    auto_expand_vi = st.checkbox("T·ª± m·ªü r·ªông t·ª´ kho√° VI‚ÜíEN", True)
    show_debug = st.checkbox("Hi·ªÉn th·ªã URL/meta truy v·∫•n", True)
    st.divider()

    st.header("‚úçÔ∏è (Tu·ª≥ ch·ªçn) Vi·∫øt b·∫±ng Gemini")
    use_gemini = st.checkbox("D√πng Gemini ƒë·ªÉ so·∫°n b√†i?", True)
    gemini_model = st.selectbox("Model", ["gemini-1.5-pro", "gemini-1.5-flash"], 0)
    author_name = st.text_input("T√°c gi·∫£ hi·ªÉn th·ªã", "Nh√≥m nghi√™n c·ª©u")
    keywords = st.text_input("T·ª´ kh√≥a", "tƒÉng tr∆∞·ªüng xanh; b·ªÅn v·ªØng; nƒÉng l∆∞·ª£ng t√°i t·∫°o; s·ªë ho√°")
    subtitle = st.text_input("Ph·ª• ƒë·ªÅ", "B√†i t·ªïng quan h·ªá th·ªëng c√≥ tr√≠ch d·∫´n h·ªçc thu·∫≠t")

    st.divider()
    run = st.button("üöÄ T·∫°o b√†i vi·∫øt")

# ================== Helpers ==================
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def year_from_date(s):
    if not s:
        return None
    try:
        return int(str(s)[:4])
    except Exception:
        return None

def doi_url(doi):
    if not doi:
        return None
    doi = doi.lower().replace("https://doi.org/", "").replace("http://doi.org/", "").strip()
    return f"https://doi.org/{doi}"

def verify_doi_head(doi: str, timeout=8) -> bool:
    if not doi:
        return False
    try:
        r = requests.head(doi_url(doi), allow_redirects=True, timeout=timeout)
        return r.status_code < 400
    except Exception:
        return False

def normalize_author_list(authors):
    if isinstance(authors, list):
        out = []
        for a in authors:
            if isinstance(a, str):
                out.append(a)
            elif isinstance(a, dict):
                name = a.get("name") or (a.get("author") or {}).get("display_name")
                if name:
                    out.append(name)
        return out
    return []

def reconstruct_openalex_abstract(inv):
    """OpenAlex c√≥ abstract_inverted_index ‚Üí gh√©p l·∫°i."""
    if not isinstance(inv, dict) or not inv:
        return ""
    positions = []
    for word, idxs in inv.items():
        for i in idxs:
            positions.append((i, word))
    positions.sort()
    return " ".join(w for _, w in positions)

def expand_query_vi_to_en(q: str) -> str:
    ql = q.lower()
    extras = []
    # Th√™m c√°c m·ªü r·ªông ph·ªï bi·∫øn (b·∫°n c√≥ th·ªÉ ch·ªânh theo lƒ©nh v·ª±c)
    if "tƒÉng tr∆∞·ªüng xanh" in ql or "green growth" in ql:
        extras += ["green growth", "green economy", "sustainable growth"]
    if "chuy·ªÉn d·ªãch nƒÉng l∆∞·ª£ng" in ql or "energy transition" in ql:
        extras += ["energy transition", "decarbonization", "low-carbon transition", "renewable energy transition"]
    if "kinh t·∫ø xanh" in ql or "green economy" in ql:
        extras += ["green economy", "circular economy", "sustainable economy"]
    if "ph√°t th·∫£i" in ql or "carbon" in ql:
        extras += ["carbon emissions", "emission reduction", "net zero", "carbon neutrality"]
    # G·ªôp b·∫£n g·ªëc + ti·∫øng Anh m·ªü r·ªông, lo·∫°i tr√πng
    parts = [q] + [e for e in extras if e not in q]
    return " ".join(parts)

@st.cache_data(show_spinner=False)
def openalex_search(topic, years, per_page=50, max_pages=3, loosen_types=True, auto_expand_vi=True, show_debug=False):
    base = "https://api.openalex.org/works"
    search_q = expand_query_vi_to_en(topic) if auto_expand_vi else topic
    params = {
        "search": search_q,
        "filter": [],
        "per_page": per_page,
        "sort": "relevance_score:desc"
    }
    if years:
        try:
            start, end = years.split("-")
            params["filter"].append(f"from_publication_date:{start}-01-01")
            params["filter"].append(f"to_publication_date:{end}-12-31")
        except ValueError:
            pass
    types = "journal-article|proceedings-article|report|book-chapter" if loosen_types else "journal-article"
    params["filter"].append(f"type:{types}")
    params["filter"] = ",".join(params["filter"])

    out = []
    cursor = "*"
    for _ in range(max_pages):
        q = params.copy()
        q["cursor"] = cursor
        url = f"{base}?{urlencode(q)}"
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        data = r.json()
        if show_debug:
            st.caption(f"üîé OpenAlex URL: {url}")
            st.caption(f"üì¶ meta: {data.get('meta', {})}")
        for it in data.get("results", []):
            title = clean_text(it.get("title"))
            abstract = clean_text(it.get("abstract")) if it.get("abstract") else reconstruct_openalex_abstract(it.get("abstract_inverted_index"))
            doi = it.get("doi")
            primary_location = it.get("primary_location") or {}
            landing = primary_location.get("landing_page_url")
            oa_url = primary_location.get("pdf_url")
            year = year_from_date(it.get("publication_year") or it.get("publication_date"))
            venue = (it.get("host_venue") or {}).get("display_name")
            authors = []
            for au in it.get("authorships", []):
                aname = (au.get("author") or {}).get("display_name")
                if aname:
                    authors.append(aname)
            out.append({
                "id": it.get("id"),
                "title": title,
                "abstract": abstract,
                "doi": doi,
                "url": landing,
                "oa_pdf_url": oa_url,
                "year": year,
                "venue": venue,
                "authors": authors
            })
        cursor = (data.get("meta") or {}).get("next_cursor")
        if not cursor:
            break
    return out

def has_valid_url(d):
    for k in ["oa_pdf_url", "url", "landing_page"]:
        if d.get(k):
            return True
    return False

def make_bibliography(sources):
    out = []
    for s in sources:
        auths = normalize_author_list(s.get("authors"))
        auth_str = "; ".join(auths) if auths else "N/A"
        year = s.get("year") or "n.d."
        title = s.get("title") or "Untitled"
        ven = s.get("venue") or ""
        doi = s.get("doi")
        link = doi_url(doi) if doi else (s.get("url") or s.get("oa_pdf_url") or "")
        out.append(f"{auth_str} ({year}). {title}. {ven}. {link}")
    return out

def make_sources_bulleted(sources):
    lines = []
    for i, s in enumerate(sources, start=1):
        title = s.get("title") or "(no title)"
        year = s.get("year")
        auths = ", ".join(normalize_author_list(s.get("authors")))
        ven = s.get("venue") or ""
        doi = s.get("doi")
        link = doi_url(doi) if doi else (s.get("url") or s.get("oa_pdf_url") or "")
        lines.append(f"[{i}] {auths} ({year}). {title}. {ven}. {link}".strip())
    return "\n".join(lines)

def enforce_citation_integrity(text, n_sources):
    used = set(int(m.group(1)) for m in re.finditer(r"\[(\d+)\]", text))
    invalid = [i for i in used if i < 1 or i > n_sources]
    fixed = text
    for bad in sorted(invalid, reverse=True):
        fixed = re.sub(rf"\[{bad}\]", "", fixed)
    return fixed

def plot_publications_by_year(df):
    fig = plt.figure()
    counts = df["year"].dropna().astype(int).value_counts().sort_index()
    if counts.empty:
        plt.title("Kh√¥ng ƒë·ªß d·ªØ li·ªáu nƒÉm")
    else:
        counts.plot(kind="bar")
        plt.title("S·ªë b√†i c√¥ng b·ªë theo nƒÉm")
        plt.xlabel("NƒÉm"); plt.ylabel("S·ªë b√†i")
        plt.tight_layout()
    return fig

def plot_top_venues(df, topk=10):
    fig = plt.figure()
    vc = df["venue"].dropna().apply(lambda s: s.strip()).value_counts().head(topk)
    if vc.empty:
        plt.title("Kh√¥ng ƒë·ªß d·ªØ li·ªáu t·∫°p ch√≠")
    else:
        vc.plot(kind="barh")
        plt.title(f"Top {topk} t·∫°p ch√≠/ngu·ªìn")
        plt.xlabel("S·ªë b√†i"); plt.ylabel("T·∫°p ch√≠/Ngu·ªìn")
        plt.tight_layout()
    return fig

MD_TEMPLATE = """---
title: "{{ title }}"
subtitle: "{{ subtitle }}"
author:
  - name: "{{ author }}"
date: "{{ date }}"
lang: vi
---

# {{ title }}

**T√°c gi·∫£:** {{ author }}

**T·ª´ kh√≥a:** {{ keywords }}

---

## 1. Gi·ªõi thi·ªáu
{{ intro }}

## 2. Ph∆∞∆°ng ph√°p (PRISMA / Systematic Review)
{{ methods }}

### 2.1 S∆° ƒë·ªì PRISMA (mermaid)
```mermaid
{{ prisma_mermaid }}
```

## 3. K·∫øt qu·∫£
{{ results }}

### 3.1 Xu h∆∞·ªõng c√¥ng b·ªë theo nƒÉm
![Xu h∆∞·ªõng c√¥ng b·ªë](fig_publications_by_year.png)

### 3.2 Top t·∫°p ch√≠/ngu·ªìn
![Top t·∫°p ch√≠](fig_top_venues.png)

## 4. Th·∫£o lu·∫≠n
{{ discussion }}

## 5. K·∫øt lu·∫≠n
{{ conclusion }}

### H·∫°n ch·∫ø
{{ limitations }}

---

## T√†i li·ªáu tham kh·∫£o
{% for i, src in enumerate(bibliography, start=1) -%}
[{{i}}] {{ src }}
{% endfor %}
"""

SYSTEM_STYLE_INSTR = """B·∫°n l√† m·ªôt nh√† nghi√™n c·ª©u (ti·∫øn sƒ©) vi·∫øt vƒÉn phong h·ªçc thu·∫≠t, m·∫°ch l·∫°c, c√≥ tr√≠ch d·∫´n theo d·∫°ng [#] ƒë√∫ng v·ªã tr√≠. Tuy·ªát ƒë·ªëi kh√¥ng ƒë∆∞·ª£c b·ªãa ngu·ªìn hay ch√®n tr√≠ch d·∫´n kh√¥ng c√≥ trong danh m·ª•c 'C√ÅC NGU·ªíN H·ª¢P L·ªÜ'. N·∫øu kh√¥ng ƒë·ªß b·∫±ng ch·ª©ng, h√£y n√≥i r√µ 'ch∆∞a ƒë·ªß b·∫±ng ch·ª©ng t·ª´ ngu·ªìn h·ª£p l·ªá' thay v√¨ suy ƒëo√°n."""

SECTION_PROMPT = """
{system}

CH·ª¶ ƒê·ªÄ CHUNG: "{topic}"

C√ÅC NGU·ªíN H·ª¢P L·ªÜ (ƒë∆∞·ª£c ph√©p tr√≠ch d·∫´n):
{sources_bulleted}

Y√äU C·∫¶U:
- Vi·∫øt ph·∫ßn: {section_title}
- Ng√¥n ng·ªØ: ti·∫øng Vi·ªát, chu·∫©n h·ªçc thu·∫≠t, r√µ r√†ng.
- D·∫´n ngu·ªìn t·∫°i ch·ªó theo d·∫°ng [#], v·ªõi # l√† s·ªë th·ª© t·ª± ƒë√∫ng c·ªßa danh m·ª•c ngu·ªìn ·ªü tr√™n (tuy·ªát ƒë·ªëi kh√¥ng tr√≠ch d·∫´n ngo√†i danh m·ª•c).
- Kh√¥ng l·∫∑p l·∫°i ti√™u ƒë·ªÅ.
- Tr√°nh s√°o r·ªóng; t·∫≠p trung v√†o b·∫±ng ch·ª©ng, tranh lu·∫≠n ch√≠nh v√† ‚Äúso s√°nh ‚Äì ƒë·ªëi chi·∫øu‚Äù.

ƒê·ªò D√ÄI G·ª¢I √ù: {length_hint} t·ª´.

B·∫ÆT ƒê·∫¶U VI·∫æT:
"""

def write_with_gemini(model_name, prompt, max_tokens=1800):
    try:
        import google.generativeai as genai
    except Exception:
        st.error("Ch∆∞a c√†i `google-generativeai`. Ch·∫°y: pip install google-generativeai")
        return ""
    api_key = os.getenv("GEMINI_API_KEY", "")
    if not api_key:
        st.warning("Thi·∫øu GEMINI_API_KEY ‚Üí ch·ªâ t·∫°o d·ªØ li·ªáu & bi·ªÉu ƒë·ªì, kh√¥ng so·∫°n vƒÉn b·∫£n.")
        return ""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    resp = model.generate_content(
        prompt,
        generation_config={"temperature": 0.4, "max_output_tokens": max_tokens}
    )
    return resp.text or ""

# ================== Main flow ==================
colL, colR = st.columns([1, 1])

if run:
    with st.spinner("ƒêang t√¨m tr√™n OpenAlex..."):
        try:
            works = openalex_search(
                topic, year_range,
                per_page=int(per_page),
                max_pages=int(max_pages),
                loosen_types=loosen_types,
                auto_expand_vi=auto_expand_vi,
                show_debug=show_debug
            )
        except Exception as e:
            st.error(f"L·ªói OpenAlex: {e}")
            works = []

    prisma = {"initial": len(works)}

    # Clean + validate
    clean = []
    seen_titles = set()
    for w in works:
        title_l = (w.get("title") or "").strip().lower()
        if not title_l or title_l in seen_titles:
            continue
        seen_titles.add(title_l)

        ok = False
        doi = w.get("doi")
        if verify_doi and doi and verify_doi_head(doi):
            ok = True
        elif has_valid_url(w) or doi:
            ok = True

        if ok:
            y = w.get("year")
            if isinstance(y, str) and y.isdigit():
                y = int(y)
            w["year"] = y
            clean.append(w)

    prisma["deduped"] = len(clean)

    # Title screening
    topic_tokens = [t.strip().lower() for t in re.split(r"[;,\s]\s*", topic) if len(t.strip()) > 2]
    title_keep = []
    for w in clean:
        t = (w.get("title") or "").lower()
        if any(tok in t for tok in topic_tokens):
            title_keep.append(w)
    if len(title_keep) < max(10, int(0.3*len(clean))):
        title_keep = clean
    prisma["screened_title"] = len(title_keep)

    # Abstract screening
    abs_keep = []
    for w in title_keep:
        ab = (w.get("abstract") or "").lower()
        if ab:
            if any(tok in ab for tok in topic_tokens):
                abs_keep.append(w)
        else:
            abs_keep.append(w)
    prisma["screened_abstract"] = len(abs_keep)

    # Limit sources
    sources = abs_keep[: int(max_sources)]
    prisma["included_fulltext"] = len(sources)

    if sources:
        df = pd.DataFrame(sources)
        st.subheader("üìö Ngu·ªìn thu th·∫≠p ƒë∆∞·ª£c (ƒë√£ l·ªçc)")
        st.dataframe(df[["title","year","venue","doi","url","oa_pdf_url"]], use_container_width=True, height=350)

        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è T·∫£i sources.csv", csv, "sources.csv", "text/csv")

        with colL:
            st.subheader("üìà Xu h∆∞·ªõng c√¥ng b·ªë theo nƒÉm")
            fig1 = plot_publications_by_year(df)
            st.pyplot(fig1, use_container_width=True)

        with colR:
            st.subheader("üè∑Ô∏è Top t·∫°p ch√≠/ngu·ªìn")
            fig2 = plot_top_venues(df, topk=10)
            st.pyplot(fig2, use_container_width=True)

        st.subheader("üß≠ PRISMA (Mermaid code)")
        prisma_mermaid = f"""flowchart TB
A[Records identified: {prisma.get('initial', 0)}] --> B[After deduplication: {prisma.get('deduped', 0)}]
B --> C[Title screening included: {prisma.get('screened_title', 0)}]
C --> D[Abstract screening included: {prisma.get('screened_abstract', 0)}]
D --> E[Full-text included: {prisma.get('included_fulltext', 0)}]
"""
        st.code(prisma_mermaid, language="mermaid")

        # Optional: generate paper with Gemini
        paper_md = ""
        if use_gemini:
            sources_bulleted = make_sources_bulleted(sources)
            bibliography = make_bibliography(sources)

            def section(title, length_hint):
                prompt = SECTION_PROMPT.format(
                    system=SYSTEM_STYLE_INSTR,
                    topic=topic,
                    sources_bulleted=sources_bulleted,
                    section_title=title,
                    length_hint=length_hint
                )
                txt = write_with_gemini(gemini_model, prompt)
                return enforce_citation_integrity(txt, len(bibliography))

            with st.spinner("Gemini ƒëang so·∫°n b√†i..."):
                intro = section("Gi·ªõi thi·ªáu: b·ªëi c·∫£nh, kh√°i ni·ªám tr·ªçng t√¢m, t·∫ßm quan tr·ªçng v√† kho·∫£ng tr·ªëng nghi√™n c·ª©u", 450)
                methods = section("Ph∆∞∆°ng ph√°p: chi·∫øn l∆∞·ª£c t√¨m ki·∫øm, ti√™u ch√≠ PRISMA, c∆° s·ªü d·ªØ li·ªáu, c√°ch ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng nghi√™n c·ª©u", 350)
                results = section("K·∫øt qu·∫£: c√°c c·ª•m ch·ªß ƒë·ªÅ, khuynh h∆∞·ªõng ƒë·ªãnh l∆∞·ª£ng, ph√°t hi·ªán ch√≠nh so v·ªõi m·ª•c ti√™u nghi√™n c·ª©u", 400)
                discussion = section("Th·∫£o lu·∫≠n: di·ªÖn gi·∫£i ph√°t hi·ªán, so s√°nh v·ªõi t√†i li·ªáu, h√†m √Ω ch√≠nh s√°ch/th·ª±c h√†nh, tranh lu·∫≠n h·ªçc thu·∫≠t", 450)
                conclusion = section("K·∫øt lu·∫≠n: t√≥m t·∫Øt ƒë√≥ng g√≥p, h∆∞·ªõng nghi√™n c·ª©u ti·∫øp theo", 220)
                limitations = section("H·∫°n ch·∫ø: d·ªØ li·ªáu, ph∆∞∆°ng ph√°p, ƒë·ªô bao ph·ªß; c√°ch kh·∫Øc ph·ª•c trong t∆∞∆°ng lai", 200)

                context = {
                    "title": f"T·ªïng quan h·ªá th·ªëng v·ªÅ {topic}",
                    "subtitle": subtitle,
                    "author": author_name,
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "keywords": keywords,
                    "intro": intro,
                    "methods": methods,
                    "results": results,
                    "discussion": discussion,
                    "conclusion": conclusion,
                    "limitations": limitations,
                    "prisma_mermaid": prisma_mermaid,
                    "bibliography": bibliography
                }
                paper_md = Template(MD_TEMPLATE).render(**context)

            st.subheader("üìù B·∫£n th·∫£o (Markdown)")
            st.code(paper_md, language="markdown")
            st.download_button("‚¨áÔ∏è T·∫£i paper.md", paper_md.encode("utf-8"), file_name="paper.md", mime="text/markdown")

        with st.expander("üí° G·ª£i √Ω xu·∫•t PDF (tu·ª≥ ch·ªçn)"):
            st.markdown("""
- D√πng **Pandoc** v·ªõi filter Mermaid ho·∫∑c render Mermaid ‚Üí PNG tr∆∞·ªõc, r·ªìi nh√∫ng h√¨nh v√†o Markdown.
- Ho·∫∑c copy kh·ªëi Markdown v√†o Obsidian/MkDocs/VS Code (Markdown Preview Enhanced) ƒë·ªÉ render Mermaid.
""")
    else:
        st.warning("Kh√¥ng thu ƒë∆∞·ª£c ngu·ªìn n√†o. H√£y n·ªõi r·ªông nƒÉm, tƒÉng `max_pages`, b·∫≠t 'N·ªõi l·ªèng lo·∫°i t√†i li·ªáu' v√† 'T·ª± m·ªü r·ªông t·ª´ kho√° VI‚ÜíEN', ho·∫∑c t·∫Øt x√°c th·ª±c DOI.")
else:
    st.info("Nh·∫≠p c·∫•u h√¨nh ·ªü thanh b√™n v√† ·∫•n **üöÄ T·∫°o b√†i vi·∫øt** ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
